<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <title>Maximum Likelihood Estimation</title>
        <link rel="canonical" href="https://eu90h.com/max-likelihood-estimation.html" />
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="https://unpkg.com/normalize.css">
        <link rel="stylesheet" href="https://unpkg.com/concrete.css">
    </head>
    <body>
        <main>
            <section>
                <h1>Maximum Likelihood Estimation</h1>
                <p>
                    Maximum Likelihood Estimation (MLE) is a method for estimating the parameters of a statistical model.
                    Suppose that we've collected a dataset \(\mathcal{D} = \{x_k\}_{k=1}^N\) and that we have good prior reason to suspect that the data is best modeled
                    by a particular family of probability distributions \(p(x|\theta)\) where \(\theta\) is that model's parameters.
                    The act of choosing a particular value for \(\theta\) is called model selection.
                </p>
                <p>
                    Suppose we had a particular \(\theta\) value. Then the likelihood of the data, given the parameters, is given by \(p(\mathcal{D}|\theta) = p(x_1x_2\dots x_N|\theta)\).
                    We can make a simplifying assumption that the data is indepedent and identically distributed (IID), so that
                    $$
                    \begin{aligned}
                    \begin{equation}
                    p(\mathcal{D}|\theta) = p(x_1x_2\dots x_N|\theta) \\
                    = \prod_{k=1}^Np(x_k|\theta)
                    \end{equation}
                    \end{aligned}
                    $$
                </p>
                <p>
                    The essence of the maximum likelihood method lies in the simple idea that we should choose our model parameter \(\theta\) to maximize the likelihood of the
                    data that we've actually observed. In practice, we're going to maximize the <em>negative log-likelihood</em>,
                    $$
                    \ell_\mathcal{D}(\theta) = -\log p(\mathcal{D}|\theta) = -\sum_{k=1}^N\log{p(x_k|\theta)}
                    $$
                    where the notation \(\ell_\mathcal{D}(\theta)\) emphasizes that the NLL is a function of the parameters with the data fixed beforehand. We'll use this as a score to rank the quality of models with respect to our datset.
                </p>
            </section>
            <section>
                <p>
                    Now let's maximize the NLL by taking the derivative and finding the critical points \(\theta^*\):
                    $$
                    \begin{equation}
                    \begin{aligned}
                    \frac{\partial}{\partial\theta}\ell_\mathcal{D}(\theta^*)
                    &= -\sum_{k=1}^N\frac{\partial}{\partial\theta}\log{p(x_k|\theta^*)} = 0\\
                    \end{aligned}
                    \end{equation}
                    $$
                    and then checking that the second derivative is negative there to ensure the point is indeed a maximum.
                </p>
            </section>
            <footer>
                <a href="index.html">home</a>
                <a href="about.html">about</a>
            </footer>
        </main>
    </body>
</html>
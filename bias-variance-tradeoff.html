<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <title>Bias-Variance Tradeoff</title>
        <link rel="canonical" href="https://eu90h.com/bias-variance-tradeoff.html" />
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="https://unpkg.com/normalize.css">
        <link rel="stylesheet" href="https://unpkg.com/concrete.css">
    </head>
    <body>
        <main>
            <section>
                <h1>Bias-Variance Tradeoff</h1>
                <p>
                    Suppose we have a sample of N observations \(x_1,x_2,\dots,x_N\) drawn independently from some distribution \(p(x|\theta)\) parameterized by \(\theta\).
                    A <em>point estimator</em> is a function of the sample data intended to estimate the value of some function \(g(\theta)\) of a parameter \(\theta\) of a statistical model.
                </p>
                <p>
                    A simple example would be the mean estimator
                    $$
                    \hat\mu = \frac{1}{n}\sum_{i=1}^n X_i
                    $$
                </p>
            </section>
            <section>
                <p>
                    The definition of point estimators (hereafter, simply <em>estimators</em>) is pretty general -- it doesn't say the estimator has to be good at estimation, just that it's a function of the data. 
                </p>
                <p>
                    This motivates the idea of a consistent estimator. An estimator is <em>consistent</em> if it converges
                    probabilistically to the true value of the parameter we're estimating.
                </p>
                <p>We define the <em>bias</em> of an estimator \(\hat\theta=T(X_1,\dots,X_N)\) for parameter \(\theta\) to be the difference between the expected estimated value and the population distribution's "true" parameter \(\theta\),
                    $$
                    \text{bias}(\hat\theta) = E[\hat\theta] - \theta.
                    $$
                    The bias can be understood as the estimator's expected error.

                </p>
                <p>
                    We say that an estimator \(\hat\theta\) is <em>unbiased</em> if it has zero bias: \(\text{bias}(\hat\theta) = 0\). An unbiased estimator correctly estimates the true parameter value, on average, as the number of samples increase.
                </p>
            </section>
            <section>
                <p>Notice how the expected value \(E[\hat\theta(X_1,\dots,X_N)]\) is computed over the possible values of the observations \(X_k\).
                    This lies at the core of frequentist statistics. Uncertainty is represented in terms of the variation across repeated trials. This variation is induced by sampling from the population distribution.
                </p>
                <p>
                    This is what is meant by \(E[\hat\theta(X_1,\dots,X_N)]\): we consider how the statistic changes over every possible sample. Following Murphy [0, pg. 150], we sample \(N_d\) datasets consisting of \(N_o\) observations from some true model \(p(x|\theta^*)\):
                    $$
                    \tilde{\mathcal{D}}^{(i)}=\{x_k\sim p(x_k|\theta^*):k=1,\dots,N\}
                    $$
                </p>
                <p>
                    Now, as we the number of datasets \(N_d\) increases unbounded, the approximation
                    $$
p(\hat\theta(\tilde{\mathcal{D}})=\theta|\tilde{\mathcal{D}}\sim\theta^*)\approx\frac{1}{N_d}\sum^{N_d}_{k=1}\mathbb{I}[\theta=\hat\theta(\tilde{\mathcal{D}}^{(k)})]
                    $$
                    approaches equality.
                </p>
            </section>
            <section>
                <p>
                    We say that the parameters \(\theta\) of a probability distribution function \(p\) are <em>identifiable</em>
                    if the pdf \(p(x|\theta)\) is injective as a function of \(\theta\). That is, \(p\) must satisfy
                    $$
                        p(\mathcal{D}|\theta_0) = p(\mathcal{D}|\theta_1) \implies \theta_0 = \theta_1
                    $$
                </p>
                <p>
                    The maximum likelihood estimator (MLE), given by
                    $$
                    \hat\theta(\mathcal{D})=\sup_\theta{p(\mathcal{D}|\theta)},
                    $$
                    has a sampling distribution that is approximately normal in the large-sample limit.
                </p>
                <p>
                    We have the theorem [0, pg. 151]:<br>
                    if the parameters of the population distribution are identifiable, then
                    $$
p(\hat\theta(\mathcal{D}) = \theta|\mathcal{D}\sim\theta^*)\rightarrow N(\theta|\theta^*, \frac{1}{N}({\text{I}(\theta)})^{-1}),
                    $$
                    where \(I\) is the Fisher information.
                </p>
            </section>
            <section>
                <p>
                    Finally, consider the mean-squared-error of some estimator \(\hat\theta\), assuming \(\theta\) is the true parameter. A little algebra shows that
                    $$
                    E[(\hat\theta - \theta^*)^2] = \text{bias}^2(\hat\theta) + V[\hat\theta]
                    $$
                    where \(V(\hat\theta)\) is the variance of the estimator. This is the <em>bias-variance tradeoff</em>.
                </p>
            </section>
            <section>
            </section>
            <section>
                <h3>References</h3>
                <ul>
                    <li>[0] Murphy, Kevin P., <em>Probabilistic Machine Learning</em>, MIT Press, Vol. 1, 2022</li>
                </ul>
            </section>
            <footer>
                <a href="index.html">home</a>
                <a href="about.html">about</a>
            </footer>
        </main>
    </body>
</html>
